{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>機械学習課題 3 : Linear Regression 回帰分析 Part 2 - Gradient Descent 最急降下法 続き</h3><br/>\n",
    "\n",
    "<h4>注意 : pythonコードを実行しながら読んでください!</h4><br/>\n",
    "\n",
    "前回の3番目課題の続きです。\n",
    "\n",
    "今回は最急降下法（Gradient Descent）を完成します。まず、スキー場での滑り方を復習しましょう。\n",
    "\n",
    "1. グラフ上の立って、そこから移動する（滑る）目的地のポイントを決める。\n",
    "2. 現在の位置から目的地までの傾きを計算し、その分移動する（滑る）\n",
    "3. 1、2を反復する。ただし、傾きがゼロになったら、もう滑れないので止める。\n",
    "\n",
    "今回も必要ですので、前回の課題を参考にして、教師データの読み込みと目的関数(Cost Function)を実装してください。あと、グラフを使って、$\\theta_0$、$\\theta_1$と$J(\\theta_0,\\theta_1)$のそれぞれの変化を可視化してみましょう。（2次元のContourを使うか3次元グラフを使うかはお任せします。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#cost function\n",
    "def costFunction(X, Y, theta0, theta1):\n",
    "    J = 0\n",
    "    \n",
    "    # ------- Coding Start -------\n",
    "\n",
    "    # ------- Coding End -------\n",
    "    return J\n",
    "\n",
    "#read learning data X and Y\n",
    "# ------- Coding Start -------\n",
    "\n",
    "# ------- Coding End -------\n",
    "\n",
    "#draw graph to visualize the relation of theta0, theta1 and cost J\n",
    "# ------- Coding Start -------\n",
    "\n",
    "# ------- Coding End -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>なんか格好いいソースコードの形になりましたね。</h4>\n",
    "\n",
    "これから、最急降下法（Gradient Descent）を完成します。\n",
    "\n",
    "ある$\\theta_0$、$\\theta_1$の位置での傾きは以下の微分式で表すことができます。\n",
    "\n",
    "$$(1) = \\frac{\\Delta}{\\Delta \\theta_0} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "$$(2) = \\frac{\\Delta}{\\Delta \\theta_1} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "$(1)$は$\\theta_0$の変化量の基準での$J$の変化量、$(2)$は$\\theta_1$の変化量の基準での$J$の変化量を表わします。つまり、この二つを合わせて、ある位置でのグラフの傾きが表現できます。\n",
    "\n",
    "<h4>問題は、これからどうやって滑っていくのかです。</h4>\n",
    "\n",
    "ある位置と次の移動先の位置との関係は以下のように表わします。\n",
    "\n",
    "$$(3) New \\theta_0 = Current \\theta_0 - \\alpha * (1)$$\n",
    "\n",
    "$$(4) New \\theta_1 = Current \\theta_1 - \\alpha * (2)$$\n",
    "\n",
    "具体的には、\n",
    "\n",
    "$$(5) New \\theta_0 = Current \\theta_0 - \\alpha * \\frac{\\Delta}{\\Delta \\theta_0} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "$$(6) New \\theta_1 = Current \\theta_1 - \\alpha * \\frac{\\Delta}{\\Delta \\theta_1} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "<br/>\n",
    "になります。この式を繰り返して進めていくと、あるところで$New$と$Current$の$\\theta$の変化がほぼなくなる位置が見つかるので、そこまで滑れば終わりです。ただし、どこに届いても傾きがゼロにならない限り、本当に止めて良いのは分からないケースがほとんどですので、実際には滑っていく回数を決めて回して見るのが一般的です。回しながらCostの$J$の変化様子を観察して何処まで滑るか判断するしかないです。\n",
    "\n",
    "ちなみに$\\alpha$は前回勉強した学習率(Learning Rate)というやつです。その値のチューニングの目的は前回勉強しましたので、思い出さない場合は再度読んでみてください。\n",
    "\n",
    "> 移動距離を長くすると、最小の傾きを探すのに時間は短縮できますが、完全な最小の値が見つからない可能性があり、その反面、移動距離を短くすると、滑っていくのに相当な時間がかかってしまう、というメリットとデメリットがあります。実際に最急降下法を適用する際に、この移動距離に相当する値を学習率(Learning Rate)と言います。記号は主に$\\alpha$を使います。実際に機械学習を応用する際には、推論モデルの正答率と機械学習の性能の間のバランスを見て$\\alpha$の値をチューニングしていく必要があります。\n",
    "\n",
    "<br/>\n",
    "<h4>微分！やってみましょう。</h4>\n",
    "\n",
    "まず、これどうしましょうか。\n",
    "\n",
    "$$\\frac{\\Delta}{\\Delta \\theta} J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "$J(\\theta_0,\\theta_1)$を微分するとどうなりますかね。実際の式を持ってきてやってみます。これが元々の$J$の式でした。\n",
    "\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m((\\theta_0 + \\theta_1 * X) - Y)^2$$\n",
    "\n",
    "$\\theta_0$と$\\theta_1$に対した$J$の微分はそれぞれ以下の(7)、(8)のようになります。\n",
    "\n",
    "$$(7) \\frac{\\Delta}{\\Delta \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^m((\\theta_0 + \\theta_1 * X) - Y) $$\n",
    "\n",
    "$$(8) \\frac{\\Delta}{\\Delta \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^m((\\theta_0 + \\theta_1 * X) - Y)X $$\n",
    "\n",
    "<br/>\n",
    "ここまでいかがでしょうか。\n",
    "\n",
    "実は、これが（式(7)、(8)）理解できなくても大丈夫です。これから実際に機械学習を応用する際に自分が直接微分する必要はありません。必要な数学ライブラリは全部提供されます。今回は概念を学ぶ為に直接微分式を実装してみますが、実際にはほぼないと思います。今まで話してきた、スキー場から滑って下りていく概念だけ理解しておけば大丈夫です。\n",
    "\n",
    "(5)、(6)の式に(7)、(8)をそれぞれ代入すると以下の(9)、(10)のようになります。\n",
    "\n",
    "$$(9) New \\theta_0 = Current \\theta_0 - \\alpha * \\frac{1}{m}\\sum_{i=1}^m((\\theta_0 + \\theta_1 * X) - Y)$$\n",
    "\n",
    "$$(10) New \\theta_1 = Current \\theta_1 - \\alpha * \\frac{1}{m}\\sum_{i=1}^m((\\theta_0 + \\theta_1 * X) - Y)X$$\n",
    "\n",
    "<h3>ここで課題です。最急降下法（Gradient Descent）をpythonで書いてみましょう。</h3>\n",
    "\n",
    "numpyライブラリを利用して簡単に作成できるはずです。numpyはコードの最初にimportしておきますので、使ってください。<br/>\n",
    "\n",
    "<h4>注意：</h4>\n",
    "\n",
    "- 学習率(Learning Rate) $alpha$と位置の移動回数$iteration$は0.01と1500回に固定しておきましたが、変えてみて色々テストしてみるのをお勧めします。\n",
    "- 以下の部分のみコードを書いてください。他の部分は修正しないでください。\n",
    "\n",
    "#------- Coding Start -------\n",
    "#------- Coding End -------\n",
    "\n",
    "<h4>コード作成後、実行すると、自動採点を行います。$Correct$が表示されると成功です。</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Function\n",
    "def gradientDescent(X, Y, theta0=0, theta1=0, alpha=0.01, iteration=1500):\n",
    "    m = Y.size\n",
    "    J_history = np.zeros(iteration)\n",
    "    T0 = theta0\n",
    "    T1 = theta1\n",
    "    \n",
    "    for i in np.arange(iteration):\n",
    "        # ------- Coding Start -------\n",
    "\n",
    "        # ------- Coding End -------\n",
    "        J_history[i] = costFunction(X, Y, T0, T1)\n",
    "    \n",
    "    return T0, T1, J_history\n",
    "\n",
    "theta0, theta1, J_history = gradientDescent(X, Y)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(value, min, max):\n",
    "    print(\"J = \", value, \" => Correct\") if value > min and value < max else print(\"J = \", value, \" => Wrong\")\n",
    "    \n",
    "print(\"Cost J Test : \")\n",
    "evaluate(J_history[-1], 0, 4.4831122)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>$Correct$が表示されましたか。おめでとうございます。</h4>\n",
    "\n",
    "これで、最急降下法（Gradient Descent）が完成できました。これを使えば、最初に読み込んできた教師データの分布に当てはまる適切な線形方程式のパラメーター$\\theta_0$、$\\theta_1$をコンピューターが見つけてくれます。\n",
    "\n",
    "もう少し理解を深める為、最急降下法実施中のCost $J$の変化をグラフで表わしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.ylabel(r'J($\\theta_0$,$\\theta_1$)')\n",
    "plt.xlabel('Iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradientDescent関数での移動回数が増えるとCost $J$も減っていく傾向が見えると思いますが、ある程度進むとCost $J$の変化がほぼ見えなくなっていることが分かります。これ以上頑張って滑ってもほぼ効果はない意味でしょう。スキー場でもほぼ平地で滑るのは相当大変ですし、面白くもないですので。\n",
    "\n",
    "これで最適な$\\theta_0$、$\\theta_1$が見つかりましたので、前々回に描いていたグラフをもう一度持ってきてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph with the almost best parameters theta0 and theta1\n",
    "plt.scatter(X, Y)\n",
    "xx = np.arange(4.5,22.5)\n",
    "yy = theta0 + theta1 * xx\n",
    "plt.plot(xx,yy, c='r') \n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ちなみに、この教師データですが、実はある国の各都市の人口とその都市の人々の平均収入との関係のデータらしいです。</h4>\n",
    "\n",
    "ですので、\n",
    "\n",
    "$$\\hat{Y} = h(X) = \\theta_0 + \\theta_1 * X$$\n",
    "\n",
    "の仮定関数のパラメーターもコンピューターが最急降下法で見つけてくれているし、あとは、ある都市の人口($X$)だけ入力すれば、その都市の人々の収入($\\hat{Y}$)を予測してくれる推論モデルが完成したという意味です。\n",
    "\n",
    "<h3>おめでとうございます！</h3>\n",
    "\n",
    "上のグラフだけ正しいラベルを付けてもう一度表示してみて終わりにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph once again with correct x,y labels\n",
    "plt.scatter(X, Y)\n",
    "xx = np.arange(4.5,22.5)\n",
    "yy = theta0 + theta1 * xx\n",
    "plt.plot(xx,yy, c='r') \n",
    "plt.xlabel(\"Population of City in 10,000s\")\n",
    "plt.ylabel(\"Profit in $10,000s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
