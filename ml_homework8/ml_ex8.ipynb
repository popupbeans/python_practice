{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>機械学習課題 8 : Logistic Regression ロジスティック回帰 Part 2 - Overall</h3><br/>\n",
    "\n",
    "<h4>注意 : pythonコードを実行しながら読んでください!</h4><br/>\n",
    "\n",
    "今回はロジスティック回帰(Logistic Regression)での目的関数(Cost Function)と最急降下法(Gradient Descent)を完成します。\n",
    "\n",
    "まず、テストの為に、データを持ってきます。データの中身もちょっと調べてみましょう。今回使うデータは、2回のテスト成績$X1$、$X2$から、ある資格取得に成功したのか或いは失敗したのかの結果$Y$をそれぞれ$1$と$0$の値で表す内容になります。\n",
    "\n",
    "今回の課題で利用予定のライブラリーを全部importしておきますので、ご確認ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read learning data X and Y\n",
    "data = np.loadtxt('ex8data1.txt', delimiter=',')\n",
    "\n",
    "df = pd.DataFrame(data,columns=['X1 : Exam 1 Score', 'X2 : Exam 2 Score', 'Y => 0 : Failed, 1 : Passed'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>このデータを学習させ、2回のテスト結果から資格取得可否が予測できるAI推論モデルを作ってみます。</h4>\n",
    "\n",
    "適切な推論モデルの形を理解する為、データをグラフで表示してみましょう。もっと分かりやすくなるはずです。資格取得に成功した場合は赤色のマルで、失敗した場合は青色のバツで表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = data[:,2] == 0\n",
    "positive = data[:,2] == 1\n",
    "\n",
    "plt.scatter(data[positive][:,0],data[positive][:,1], marker = 'o', c = 'r')\n",
    "plt.scatter(data[negative][:,0],data[negative][:,1], marker = 'x', c = 'b')\n",
    "\n",
    "xx = np.arange(30,100)\n",
    "yy = -1 * xx + 130\n",
    "plt.plot(xx, yy, c = 'g')\n",
    "\n",
    "plt.xlabel(\"X1 : Exam 1 Score\")\n",
    "plt.ylabel(\"X2 : Exam 2 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>一つ注目してもらいたいのは、真ん中の緑色の線です。</h4>\n",
    "\n",
    "一応適当に描いてみた線ですが、この線を基準に右と左（或いは上と下）でほとんどのデータが分離されています。この線のように、全体データを二つ以上の違うグループに分離する役割をする線を決定境界 (decision boundary)と呼びましょう。実際には仮定関数を構成している$h(X) = \\theta_0 + \\theta_1 * X_1 + \\theta_2 * X_2 + ... + \\theta_n * X_n$自体が決定境界になります。\n",
    "\n",
    "上に緑色の線は適当に描いた物ですから、完全ではないです。まだ赤いマルが線の下にあったり、青色のバツが線の上にあったりします。これからもっと適切な決定限界を作ってみましょう。\n",
    "\n",
    "<h4>ここでロジスティック回帰(Logistic Regression)を使った機械学習は適切な決定境界 (decision boundary)を見つけるのと同じ意味です。</h4>\n",
    "\n",
    "まず、ロジスティック回帰(Logistic Regression)での目的関数(Cost Function)を定義してみましょう。ここでは、回帰分析(Linear Regression)とは違って、学習データの$Y$は$0$或いは$1$にしかならないです。前回学んだシグモイド関数(Sigmoid Function)を考えると\n",
    "\n",
    "$$g(X) = \\hat{Y} = \\frac{1}{1 + e^{-h(X)}}$$\n",
    "\n",
    "1. $Y$が$1$の場合は、$\\hat{Y}$も$1$に近づくのが良いので、（近づくというのはコストがミニマムに近づくこと）以下のように表すことが可能です。($\\hat{Y}$が$1$になるか、$1$より小さい為、logを計算すると$0$かマイナスになる為、$log$にマイナスを付けています。)\n",
    "\n",
    "$$Cost = -log{(\\hat{Y})} = -log{(g(X))}$$\n",
    "\n",
    "2. $Y$が$0$の場合は、$\\hat{Y}$も$0$に近づくのが良いので、（近づくというのはコストがミニマムに近づくこと）以下のように表すことが可能です。（上と同じ理由で$log$にマイナスを付けています。）\n",
    "\n",
    "$$Cost = -log{(1 - \\hat{Y})} = -log{(1 - g(X))}$$\n",
    "\n",
    "1と2を合わせると$m$個の学習データに対して、目的関数(Cost Function)は以下のように表すことができます。\n",
    "\n",
    "$$J = - \\frac{1}{m}\\sum_{i=1}^m(Y^{i}log{(g(X^{i}))} + (1 - Y^{i})log{(1 - g(X^{i}))})$$\n",
    "\n",
    "この式でシグマの中には、$Y^{i}$が$1$の場合、$log{(g(X^{i}))}$のみ、$Y^{i}$が$0$の場合、$log{(1 - g(X^{i}))}$のみ残ります。$log$のインプットは1に近づくとゼロに近づきますので、この式で$J$の値をミニマムに近づくようにすれば良いです。\n",
    "\n",
    "<br/>\n",
    "\n",
    "次は、最急降下法 (Gradient Descent)です。回帰分析(Linear Regression)の時とほぼ同じです。\n",
    "\n",
    "$$New \\theta_j = Current \\theta_j - \\alpha * \\frac{\\Delta}{\\Delta \\theta_j} J = Current \\theta_j - \\alpha * \\frac{1}{m} * \\sum_{i=1}^m(g(X^i) - Y^i)X_j^i$$\n",
    "\n",
    "<h3>ここで課題です。ロジスティック回帰(Logistic Regression)を用いた機械学習プログラムを作成してみましょう。</h3>\n",
    "\n",
    "シグモイド関数（Sigmoid Function）、目的関数(Cost Function)、最急降下法 (Gradient Descent)を全部実装してください。必要なライブラリは最初にimportしておきましたので、使ってください。<br/>\n",
    "\n",
    "<h4>コード作成後、実行すると、自動採点を行います。$Correct$が表示されると成功です。</h4>\n",
    "\n",
    "<h4>注意：</h4>\n",
    "\n",
    "- 以下の部分にコードを書いてください。\n",
    "\n",
    "#------- Coding Start -------\n",
    "#------- Coding End -------\n",
    "\n",
    "- 6回目の課題で練習した通り、行列演算を前提とします。（6回目の結果を参考にしても問題ありません。ほぼ同じパターンです。）\n",
    "- シグモイド関数（Sigmoid Function）は前回の課題で作成したコードを使ってください。\n",
    "- numpyのexp、dot、log関数を利用してください。\n",
    "<br/>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html <br/>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html <br/>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data form for matrix calculation\n",
    "X = np.c_[np.ones((data.shape[0],1)),data[:,0:2]]\n",
    "Y = np.c_[data[:,2]]\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    y = z # initialize value to avoid error\n",
    "       \n",
    "    #------- Coding Start -------\n",
    "\n",
    "    #------- Coding End -------\n",
    "    \n",
    "    return np.asarray(y,dtype=np.float64)\n",
    "\n",
    "# Cost Function\n",
    "def costFunction(theta, X, Y):\n",
    "    J = 0\n",
    "    m = Y.size\n",
    "        \n",
    "    g = sigmoid(X.dot(theta))\n",
    "      \n",
    "    # ------- Coding Start -------\n",
    "\n",
    "    # ------- Coding End -------\n",
    "         \n",
    "    if np.isnan(J) :\n",
    "        return (np.inf)\n",
    "    return (J)\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradientDescent(theta, X, Y, alpha = 0.002, iteration = 500000):\n",
    "    m = Y.size\n",
    "    J_history = np.zeros(iteration)\n",
    "    Tm = theta\n",
    "    \n",
    "    for i in np.arange(iteration):\n",
    "        \n",
    "        g = sigmoid(X.dot(Tm))\n",
    "          \n",
    "        # ------- Coding Start -------\n",
    "\n",
    "        # ------- Coding End -------\n",
    "        J_history[i] = costFunction(Tm, X, Y)\n",
    "        \n",
    "        # print Gradient Descent's progress\n",
    "        progress = (i+1) / iteration * 100\n",
    "        if (progress >= 10) and ((progress % 10) == 0) :\n",
    "            print(\"Gradient Descent Progress : %3d%%\"%progress)\n",
    "    \n",
    "    return Tm, J_history\n",
    "\n",
    "theta = np.zeros((X.shape[1],1))\n",
    "theta, J_history = gradientDescent(theta, X, Y)\n",
    "\n",
    "print(\"theta :\",theta.flatten())\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Evaluation\n",
    "#-----------------------------------------------------------\n",
    "def evaluateGD(value, min, max):\n",
    "    print(\"J = \", value, \" => Correct\") if value > min and value < max else print(\"J = \", value, \" => Wrong\")\n",
    "    \n",
    "print(\"Cost J Test : \")\n",
    "evaluateGD(J_history[-1], 0, 0.2214856)\n",
    "\n",
    "plt.plot(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果はいかがですか。少し時間がかかっていたのが気になります。時間がかかるので学習の進捗が表示できるようにしておきました。\n",
    "\n",
    "<h4>このくらいの学習処理でこんなに時間がかかるなんて、あり得ない状況ですね。</h4>\n",
    "\n",
    "解決案について、次回の課題で考えてみましょう。今回の課題では、皆さんが作ったコードをもとに出てきた結果を可視化してみて終わりにします。上に描いたグラフよりは良くなっていることが確認できると思いますが、まだ間違って分類されている物が残っているあるようですね。推論モデルの正答率の確認機能についても次回の課題で実装してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = data[:,2] == 0\n",
    "positive = data[:,2] == 1\n",
    "\n",
    "plt.scatter(data[positive][:,0],data[positive][:,1], marker = 'o', c = 'r')\n",
    "plt.scatter(data[negative][:,0],data[negative][:,1], marker = 'x', c = 'b')\n",
    "\n",
    "xx = np.arange(30,100)\n",
    "#yy = -1 * xx + 130\n",
    "yy = -1 * (1 * theta[0] + xx * theta[1]) / theta[2] # newly added to visualize the generated theta \n",
    "plt.plot(xx, yy, c = 'g')\n",
    "\n",
    "plt.xlabel(\"X1 : Exam 1 Score\")\n",
    "plt.ylabel(\"X2 : Exam 2 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お疲れ様でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
