{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ある友人の話を少しします。</h3>\n",
    "\n",
    "A君は、すごく勉強が好きな友達です。全然遊ばず本ばかり読んでいたので、何でも知っている、歩く辞書みたいな人です。自分の高い知識レベルにプライドも持っているし成績も良かったので、A君には明るい未来の道が開いていると思っていました。\n",
    "\n",
    "しかし、問題は彼が社会人になってから起こってしまいます。\n",
    "\n",
    "社会では学校では学んでなかった色々なことが起こっています。たまには自分の知識を超えて想像力を使ったり、自分が正しいと思っていたことをひっくり返してみないと分からないこともたくさんあります。A君には、自分が知っている知識を全部使っても全然分からないことが増えてきました。何かあったら関連書籍を全部読んでしまいます。それでもよく分かりません。\n",
    "\n",
    "人には知識だけではなく知恵が必要です。社会人の知恵というのには、本だけではなく人と人との関係や色んな経験により形成される部分も非常に重要です。A君は残念ながら、本の情報の頼りすぎて、知恵と言えるほどの能力がまだ発達していなかったです。\n",
    "\n",
    "皆さんの周りにはこんな知り合いはいないですか。\n",
    "\n",
    "特に、いわゆる専門家と呼ばれている人たちの中に、自分の分野の常識や知識に頼りすぎて、他の人の意見や理論などに納得してくれないケースも多いでしょう。この場合は会話するのも結構難しいです。\n",
    "\n",
    "バランスの良い知識、経験をもち、考え方にもゆとりのある人たちこそ、社会人としての活躍が期待できると思います。\n",
    "\n",
    "実は、人間と同じく機械学習にもA君みたいなケースがあります。いわゆる過学習（Overfitting）ということです。作成した推論モデルが学習用のデータにフィットしすぎて、学習データのパターンから少しずれても良い結果が出にくくなることを意味します。\n",
    "\n",
    "人間が過学習状態になっていたら、どうすればいいでしょうか。勉強ばかりではなく、頭を冷やしてどこかで遊んでみたり、旅行でも行ってみるのも良いでしょう。自分の分野ではない違う分野を覗いてみるのも良いと思います。自分の意見に反対していた人たちの論理を心を開いてゆっくり考えてみるのは、とても勇気が必要ですが、自分の成長にもすごく良い方法だと思います。\n",
    "\n",
    "<h4>ちなみに、機械学習での過学習解決にはそこまでの複雑な行動は入りません。正則化（Regularization）という数学的な簡単な解決方法があるからです。</h4>\n",
    "\n",
    "それ以外にももっとたくさんのデータを学習に使うのも過学習状態を解決する手段になりますが、今回は正則化（Regularization）について練習してみましょう。\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>機械学習課題 11 : Logistic Regression ロジスティック回帰 Part 4 - Regularization 正則化</h3><br/>\n",
    "\n",
    "<h4>注意 : pythonコードを実行しながら読んでください!</h4><br/>\n",
    "\n",
    "以前学んだロジスティック回帰（Logistic Regression）の目的関数(Cost Function)を復習しましょう。確かにこんな形でしたね。（忘れてしまったら8回目の課題を参考にしてください。）\n",
    "\n",
    "$$J = - \\frac{1}{m}\\sum_{i=1}^m(Y^{i}log{(g(X^{i}))} + (1 - Y^{i})log{(1 - g(X^{i}))})$$\n",
    "\n",
    "過学習状態になった場合、シグモイド関数(Sigmoid Function)$g(X)$を構成する推論モデルのマラメーターたち、$\\theta$の量が大きくなっているため、結果的には学習データに対して、シグモイド関数(Sigmoid Function)のアウトプットが0か1にすぐ近づきやすい状態になっていると思われます。\n",
    "\n",
    "この状態では、$J$の値がすぐミニマムに近づいてしまうため、それを抑止しないと行けません。一つ良い方法は、$\\theta$の量が増加しようとした場合、目的関数(Cost Function)がミニマムに近づき難くすることです。目的関数をちょっと変えてみます。こんな感じでいかがでしょう。\n",
    "\n",
    "$$J = - \\frac{1}{m}\\sum_{i=1}^m(Y^{i}log{(g(X^{i}))} + (1 - Y^{i})log{(1 - g(X^{i}))}) + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_{j}^2$$\n",
    "\n",
    "この設計ですと、$\\theta$の量が大きくなればなるほど$\\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_{j}^2$の部分も大きくなるため、$J$は小さくならないです。その結果、$\\theta$の量が大きくなることを抑止し、過学習が防止されます。これを正則化（Regularization）と言います。\n",
    "\n",
    "ちなみに、ここで新しく登場した$\\lambda$は正則化パラメーターというものです。$\\lambda$が$0$の場合、正則化（Regularization）しないことと一緒でしょう。$\\lambda$の値を調整しながら、正則化（Regularization）の効果も調整が可能です。\n",
    "\n",
    "正則化（Regularization）を実際のコードに反映するためには、最急降下法 (Gradient Descent)の部分にも正則化（Regularization）が動くようにします。結局、$\\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_{j}^2$の部分まで微分する形になります。\n",
    "\n",
    "$$\\frac{\\Delta}{\\Delta \\theta_j} J = \\frac{1}{m} * \\sum_{i=1}^m(g(X^i) - Y^i)X_j^i + \\frac{\\lambda}{m} \\theta_{j}$$\n",
    "\n",
    "<h3>ここで課題です。前回の課題（10回目）に正則化（Regularization）を含めて、ロジスティック回帰（Logistic Regression）の機械学習コードを完成しましょう。</h3>\n",
    "\n",
    "必要なライブラリは最初にimportしておきますので、使ってください。<br/>\n",
    "\n",
    "<h4>コード作成後、実行すると、自動採点を行います。4回$Correct$が表示されると成功です。（コードが正しくなくても一回だけ$Correct$が表示されると思います。）</h4>\n",
    "\n",
    "<h4>注意：</h4>\n",
    "\n",
    "- 以下の部分にコードを書いてください。\n",
    "\n",
    "#------- Coding Start -------\n",
    "#------- Coding End -------\n",
    "\n",
    "<h4>ここまで頑張った皆さんへのプレゼントがあります。ここで、9、10回目課題のコード課題のヒントを公開します！参考にしてください。</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#read learning data X and Y\n",
    "data = np.loadtxt('ex10data1.txt', delimiter=',')\n",
    "\n",
    "# Change data form for matrix calculation\n",
    "# Get X with transformed features degree of 6 -> Generates 28 demensional data from 2 demensions\n",
    "poly = PolynomialFeatures(6)\n",
    "X = poly.fit_transform(data[:,0:2])\n",
    "# Get Y\n",
    "Y = np.c_[data[:,2]]\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    \n",
    "    y = z # initialize value to avoid error          \n",
    "    y = 1 / (1 + np.exp(-z))\n",
    "    return np.asarray(y,dtype=np.float64)\n",
    "\n",
    "# Cost Function\n",
    "def costFunctionReg(theta, lambdaParam, X, Y):\n",
    "    J = 0\n",
    "    m = Y.size\n",
    "    reg = 0\n",
    "    \n",
    "    g = sigmoid(X.dot(theta))\n",
    "    \n",
    "    # ------- Coding Start -------\n",
    "    \n",
    "    # ------- Coding End -------\n",
    "    \n",
    "    J = -1 * (1/m) * (np.log(g).T.dot(Y) + np.log(1 - g).T.dot(1- Y)) + reg\n",
    "         \n",
    "    if np.isnan(J[0]) :\n",
    "        return (np.inf)\n",
    "    return (J[0])\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradientDescentReg(theta, lambdaParam, X, Y):\n",
    "    m = Y.size\n",
    "    grad = []\n",
    "    reg = 0\n",
    "  \n",
    "    g = sigmoid(X.dot(theta.reshape(-1,1)))\n",
    "    \n",
    "    # ------- Coding Start -------\n",
    "    \n",
    "    # ------- Coding End -------\n",
    "    \n",
    "    grad = (1 / m) * X.T.dot(g - Y) + reg\n",
    "       \n",
    "    return (grad.flatten())\n",
    "\n",
    "# Call minimize function - Learning by minimize function from Scikit\n",
    "def regularizationTest(lambdaParam):\n",
    "    init_theta = np.zeros(X.shape[1])\n",
    "    res = minimize(costFunctionReg, init_theta, args=(lambdaParam,X,Y), method=None, jac=gradientDescentReg, options={'maxiter':3000})\n",
    "\n",
    "    # draw scatter graph\n",
    "    negative = data[:,2] == 0\n",
    "    positive = data[:,2] == 1\n",
    "    \n",
    "    plt.scatter(data[positive][:,0],data[positive][:,1], marker = 'o', c = 'r')\n",
    "    plt.scatter(data[negative][:,0],data[negative][:,1], marker = 'x', c = 'b')\n",
    "\n",
    "    plt.title(\"Decision boundary with regularization parameter %d\"%lambdaParam)\n",
    "    plt.xlabel(\"X1 : Exam 1 Score\")\n",
    "    plt.ylabel(\"X2 : Exam 2 Score\")\n",
    "\n",
    "    # draw contour graph\n",
    "    #create a meshgrid of theta0 and theta1\n",
    "    X1 = np.linspace(-1, 1.2, 50) # range of X1 from -1 to 1.2\n",
    "    X2 = np.linspace(-1, 1.2, 50) # range of X2 from -1 to 1.2\n",
    "    xx, yy = np.meshgrid(X1, X2, indexing='xy')\n",
    "\n",
    "    #get h(x) result from all possible values of X1 and X2\n",
    "    zz = np.zeros((X1.size, X2.size))\n",
    "    for (i,j),v in np.ndenumerate(zz):\n",
    "        temp = np.array([xx[i,j], yy[i,j]], ndmin=2)\n",
    "        #poly = PolynomialFeatures(6)\n",
    "        zz[i,j] = poly.fit_transform(temp).dot(res.x)\n",
    "    \n",
    "    plt.contour(xx,yy,zz,0,colors='g')\n",
    "    plt.show()\n",
    "        \n",
    "    return res.x\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Evaluation\n",
    "#-----------------------------------------------------------\n",
    "def evaluateReg(value, min, max):\n",
    "    print(\"Theta[1] = \", value, \" => Correct\") if value > min and value < max else print(\"Theta[1] = \", value, \" => Wrong\")\n",
    "        \n",
    "evaluateReg(regularizationTest(0)[1],44.11915492,44.11915493)  \n",
    "evaluateReg(regularizationTest(1)[1],0.63244484,0.63244485)\n",
    "evaluateReg(regularizationTest(10)[1],-0.0595276606,-0.0595276605)\n",
    "evaluateReg(regularizationTest(100)[1],-0.01683942,-0.01683941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>いかがでしょうか。</h4>\n",
    "\n",
    "正則化パラメーターの調整による決定境界形の変化を見てください。あと、$\\theta_1$の値の変化も見てください。正則化パラメーター$\\lambda$のサイズが大きくなればなるほど、正則化（Regularization）の効果も高まり、$\\theta_1$の量が小さくなっていくことが見えると思います。\n",
    "\n",
    "ちなみに、正則化（Regularization）の効果は定量化してちゃんと評価すべきですので、以下の概念も理解していただき、使ってください。（前回の課題の例を使って説明します。）\n",
    "\n",
    "- True Positive (TP) : 良品を良品として判断する数\n",
    "- True Negative (TN) : 不良品を不良品として判断する数\n",
    "- False Positive (FP) : 不良品を良品として判断する数\n",
    "- False Negative (FN) : 良品を不良品として判断する数\n",
    "\n",
    "実際にはTP、TN、FP、FNを使って、機械学習の正答率を以下の方法で評価しています。一般的に考えられる評価法は全体のデータに対して、学習データと一致した結果が出た部分の比、いわゆるAccuracyを考えればいいと思います。\n",
    "\n",
    "- Accuracy : 全体に対した正しい予測値の比です。（既に以前の課題で実装していただいています。）\n",
    "<br/><br/>\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "<br/><br/>\n",
    "\n",
    "しかし、たまにはAccuracyだけでは良く分からない部分もあります。具体的には推論モデルが良品の判別に強いのか、或いは不良品の判別の強いのか、Accuracyだけでは良く分からないです。両方同じ程度の際にはAccuracyは強力な評価ツールになりますが、そうではないと、もっと具体的に評価しないといけません。そのために使うのが以下のPrecision、Recall、F1 scoreという評価法です。\n",
    "\n",
    "- Precision : 良品と判断した数に対した本当の良品の比です。ここで評価したいのはFalse Positiveの状況です。推論モデルが良品と判断しただけで、どこまで信じていいかの評価が可能です。\n",
    "<br/><br/>\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "<br/><br/>\n",
    "- Recall (Sensitivity) : 実際の良品に対して、推論モデルが正しく良品として判断してくれた部分の比です。ここで評価したいのはFalse Negativeの状況です。\n",
    "<br/><br/>\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "<br/><br/>\n",
    "- F1 score : PrecisionとRecallの加重平均です。PrecisionとRecallがバランス良くなっているのかの評価に使います。つまり、False PositiveとFalse Negativeのアンバランス状態になっていないのかの評価が可能ですので、Accuracyより強力な評価が可能です。（PrecisionとRecall自体が高くてもバランスが良くない場合はバランス良いときよりこのスコアが低くなると思います。）\n",
    "<br/><br/>\n",
    "$$F1 score =  \\frac{2 * (Precision * Recall)}{Precision + Recall}$$\n",
    "<br/><br/>\n",
    "\n",
    "上記の各評価法について何をどうみるかは各モデルの利用用途によって変わると思います。例えば、癌の検知が必要となる推論モデルですと、Recallが低いと実際の癌患者を癌と判断できなく、助けられない人が多い意味でしょう。Precisionが低いと、癌ではないのに癌だと判断してしまい、癌治療を受けてしまう人が多いことになると思います。\n",
    "\n",
    "<h3>ここで課題です。Accuracy、Precision、Recall、F1 scoreを計算するコードを完成しましょう。</h3>\n",
    "\n",
    "必要なライブラリは最初にimportしておきますので、使ってください。<br/>\n",
    "\n",
    "<h4>コード作成後、実行すると、自動採点を行います。4回$Correct$が表示されると成功です。</h4>\n",
    "\n",
    "<h4>注意：</h4>\n",
    "\n",
    "- 以下の部分にコードを書いてください。\n",
    "\n",
    "#------- Coding Start -------\n",
    "#------- Coding End -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest(theta, X, Y, threshold = 0.5):\n",
    "    tp = 1 # initialize with 1 to avoid divided by zero error\n",
    "    tn = 1 # initialize with 1 to avoid divided by zero error\n",
    "    fp = 1 # initialize with 1 to avoid divided by zero error\n",
    "    fn = 1 # initialize with 1 to avoid divided by zero error\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    f1score = 0.0\n",
    "    \n",
    "    p = sigmoid(X.dot(theta.T)) >= threshold\n",
    "\n",
    "    #------- Coding Start -------\n",
    "\n",
    "    #------- Coding End -------\n",
    "         \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1score = 2 * (recall * precision) / (recall + precision) \n",
    "    \n",
    "    return accuracy, precision, recall, f1score\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Evaluation\n",
    "#-----------------------------------------------------------\n",
    "def evaluateTestMethod(value, min, max):\n",
    "    print(\"Accuracy = \", value[0], \" => Correct\") if value[0] > min[0] and value[0] < max[0] else print(\"Accuracy = \", value[0], \" => Wrong\")\n",
    "    print(\"Precision = \", value[1], \" => Correct\") if value[1] > min[1] and value[1] < max[1] else print(\"Precision = \", value[1], \" => Wrong\")\n",
    "    print(\"Recall = \", value[2], \" => Correct\") if value[2] > min[2] and value[2] < max[2] else print(\"Recall = \", value[2], \" => Wrong\")\n",
    "    print(\"F1 score = \", value[3], \" => Correct\") if value[3] > min[3] and value[3] < max[3] else print(\"F1 score = \", value[3], \" => Wrong\")\n",
    "\n",
    "min = [0.79661016949152, 0.73611111111111, 0.91379310344827, 0.81538461538461]\n",
    "max = [0.79661016949153, 0.73611111111112, 0.91379310344828, 0.81538461538462]\n",
    "evaluateTestMethod(modelTest(regularizationTest(6), X, Y),min,max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>結果はいかがでしょうか。</h4>\n",
    "今回の課題はここまでです。大変お疲れ様でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
